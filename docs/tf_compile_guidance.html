<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <link rel="stylesheet" href="assets/css/style.css">
    <script src="assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header  class="without-description" >
        <h1 style="color:#FFFFFF;text-shadow:none"><a href=index.html>Intel® Movidius™ Neural Compute SDK</a></h1>
        
        <p class="view"><a href="">View the Project on GitHub <small></small></a></p>
        <ul>
        
          <li>  <a href=TOC.html><small>Table of Contents</small></a> </li>
          <li>  <a href=index.html#NcSdkTools><small>NCSDK<br>Tools</small></a> </li>
          <li> <a href=ncapi/readme.html><small>NCAPI Documentation</small></a></li>
          <li>  <a href="">View On <strong>GitHub</strong></a></li>
          <li> <a href="https://ncsforum.movidius.com/">User Forum</a></li>
        </ul>
      </header>

      <section>

      <h1 id="guidance-for-compiling-tensorflow-networks">Guidance for Compiling TensorFlow™ Networks</h1>
<p>Below you will find general guidance for compiling a TensorFlow™ network that was built for training rather than inference.  The general guidance is illustrated with changes to make to the <a href="https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_deep.py">mnist_deep.py available from the TensorFlow™ GitHub repository</a>.  The changes are shown as typical diff output where a ‘-‘ at the front of a line indicates the line is removed, and a ‘+’ at the front of a line indicates the line should be added.  Lines without a ‘-‘ or ‘+’ are unchanged and provided for context.</p>

<p>In order to compile a TensorFlow™ network for use with the Neural Compute API you will need to save a version of the network that is specific to deployment/inference and omits the training features.  The following list of steps includes what users need to do to compile a typical TensorFlow™ network.  Every step may not apply to every network, but should be taken as general guidance.</p>

<ul>
  <li>Make sure there is a name set for the first layer of the network.  This is not strictly required but makes compiling much easier because if you don’t explicitly name the first and last layer you will need to determine what name those layers were given and provide those to the compiler.  For mnist_deep.py you would make the following change for the first node to give it the name “input”:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="c"># Create the model</span>
<span class="o">-</span>  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
<span class="o">+</span>  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">"input"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Add TensorFlow™ code to save the trained network.  For mnist_deep.py the change to save the trained network is:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">+</span>  <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
<span class="o">+</span>
   <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
<span class="o">...</span>

   <span class="k">print</span><span class="p">(</span><span class="s">'test accuracy </span><span class="si">%</span><span class="s">g'</span> <span class="o">%</span> <span class="n">accuracy</span><span class="o">.</span><span class="nb">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
       <span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}))</span>
<span class="o">+</span>
<span class="o">+</span>  <span class="n">graph_location</span> <span class="o">=</span> <span class="s">"."</span>
<span class="o">+</span>  <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">graph_location</span> <span class="o">+</span> <span class="s">"/mnist_model"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Run the code to train the network and make sure saver.save() is called to save the trained network.  After the program completes, if it was successful, saver.save() will have created the following files:
    <ul>
      <li>mnist_model.index</li>
      <li>mnist_model.data-00000-of-00001</li>
      <li>mnist_model.meta</li>
    </ul>
  </li>
  <li>Remove training specific code from the network, and add code to read in the previously saved network to create an inference only version.  For this step its advised that you copy the original TensorFlow™ code to a new file and modify the new file.  For example if you are working with mnist_deep.py you could copy that to mnist_deep_inference.py.  Things to remove from the inference code are:
    <ul>
      <li>Dropout layers</li>
      <li>Training specific code
        <ul>
          <li>Reading or importing training and testing data</li>
          <li>Cross entropy/accuracy code</li>
          <li>Placeholders except the input tensor.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>The ncsdk compiler does not resolve unknown placeholders.  Often extra placeholders are used for training specific variables so they are not necessary for inference.  Placeholder variables that cannot be removed should be replaced by constants in the inference graph.</p>

<p>For mnist_deep.py you would make the following changes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tempfile</span> 
<span class="o">-</span>  <span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>

<span class="o">...</span>
<span class="o">-</span>  <span class="c"># Dropout - controls the complexity of the model, prevents co-adaptation of</span>
<span class="o">-</span>  <span class="c"># features.</span>
<span class="o">-</span>  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'dropout'</span><span class="p">):</span>
<span class="o">-</span>    <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="o">-</span>    <span class="n">h_fc1_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">h_fc1</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>

<span class="o">...</span>

<span class="o">-</span>    <span class="n">y_conv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_fc1_drop</span><span class="p">,</span> <span class="n">W_fc2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fc2</span>
<span class="o">-</span>  <span class="k">return</span> <span class="n">y_conv</span><span class="p">,</span> <span class="n">keep_prob</span>
<span class="o">+</span>    <span class="n">y_conv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_fc1</span><span class="p">,</span> <span class="n">W_fc2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fc2</span>
<span class="o">+</span>  <span class="k">return</span> <span class="n">y_conv</span>
  
<span class="o">...</span>

<span class="o">-</span>  <span class="c"># Import data</span>
<span class="o">-</span>  <span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">...</span>

<span class="o">-</span>  <span class="c"># Define loss and optimizer</span>
<span class="o">-</span>  <span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="o">...</span>

   <span class="c"># Build the graph for the deep net</span>
<span class="o">-</span>  <span class="n">y_conv</span><span class="p">,</span> <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">deepnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">+</span>  <span class="c"># No longer need keep_prob since removing dropout layers.</span>
<span class="o">+</span>  <span class="n">y_conv</span> <span class="o">=</span> <span class="n">deepnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="o">...</span>

<span class="o">-</span>  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'loss'</span><span class="p">):</span>
<span class="o">-</span>    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">y_</span><span class="p">,</span>
<span class="o">-</span>                                                            <span class="n">logits</span><span class="o">=</span><span class="n">y_conv</span><span class="p">)</span>
<span class="o">-</span>  <span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
<span class="o">-</span>  
<span class="o">-</span>  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'adam_optimizer'</span><span class="p">):</span>
<span class="o">-</span>    <span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
<span class="o">-</span>  
<span class="o">-</span>  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">):</span>
<span class="o">-</span>    <span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_conv</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="o">-</span>    <span class="n">correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="o">-</span>  <span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">correct_prediction</span><span class="p">)</span>
<span class="o">-</span>
<span class="o">-</span>  <span class="n">graph_location</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
<span class="o">-</span>  <span class="k">print</span><span class="p">(</span><span class="s">'Saving graph to: </span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">graph_location</span><span class="p">)</span>
<span class="o">-</span>  <span class="n">train_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="n">graph_location</span><span class="p">)</span>
<span class="o">-</span>  <span class="n">train_writer</span><span class="o">.</span><span class="n">add_graph</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">())</span>
<span class="o">+</span>  
<span class="o">+</span>   <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>
<span class="o">+</span>
   <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
       <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
<span class="o">+</span>      <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">local_variables_initializer</span><span class="p">())</span>
<span class="o">+</span>      <span class="c"># read the previously saved network.</span>
<span class="o">+</span>      <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">'.'</span> <span class="o">+</span> <span class="s">'/mnist_model'</span><span class="p">)</span>
<span class="o">+</span>      <span class="c"># save the version of the network ready that can be compiled for NCS</span>
<span class="o">+</span>      <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">'.'</span> <span class="o">+</span> <span class="s">'/mnist_inference'</span><span class="p">)</span>

<span class="o">-</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5000</span><span class="p">):</span>
<span class="o">-</span>    <span class="n">batch</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="o">-</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="o">-</span>      <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="nb">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
<span class="o">-</span>          <span class="n">x</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})</span>
<span class="o">-</span>      <span class="k">print</span><span class="p">(</span><span class="s">'step </span><span class="si">%</span><span class="s">d, training accuracy </span><span class="si">%</span><span class="s">g'</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">))</span>
<span class="o">-</span>    <span class="n">train_step</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_</span><span class="p">:</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
<span class="o">-</span>    
<span class="o">-</span>    <span class="k">print</span><span class="p">(</span><span class="s">'test accuracy </span><span class="si">%</span><span class="s">g'</span> <span class="o">%</span> <span class="n">accuracy</span><span class="o">.</span><span class="nb">eval</span><span class="p">(</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
<span class="o">-</span>        <span class="n">x</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}))</span>
<span class="o">-</span>    <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">"./model.ckpt"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Make sure the last node is named.  As with the first node, this is not strictly required but you need to know the name to compile it.  This is the change to make to mnist_deep.py in order to have a last softmax layer with a node name of “output”:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="c"># Build the graph for the deep net</span>
<span class="o">-</span>  <span class="n">y_conv</span><span class="p">,</span> <span class="n">keep_prob</span> <span class="o">=</span> <span class="n">deepnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">+</span>  <span class="n">y_conv</span> <span class="o">=</span> <span class="n">deepnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">+</span>  <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_conv</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'output'</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Run the inference version of the code to save a session that is suitable for compiling via the ncsdk compiler.  This will only take a second since its not actually training the network, just re-saving it in an NCS-friendly way.  After you run, if successful, the following files will be created.
    <ul>
      <li>mnist_inference.index</li>
      <li>mnist_inference.data-00000-of-00001</li>
      <li>mnist_inference.meta</li>
    </ul>
  </li>
  <li>Compile the final saved network with the following command and if it all works you should see the mnist_inference.graph file created in the current directory.  Note you pass in only the weights file prefix “mnist_inference” for the -w option for a TensorFlow™ network on the compile command line.  The full command is below.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mvNCCompile mnist_inference.meta <span class="nt">-s</span> 12 <span class="nt">-in</span> input <span class="nt">-on</span> output <span class="nt">-o</span> mnist_inference.graph
</code></pre></div></div>


      </section>
    </div>
    <footer>
    
      
    </footer>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    
  </body>
</html>
